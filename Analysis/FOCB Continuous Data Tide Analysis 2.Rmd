---
title: "FOCB Continuous Data Preliminary Analysis"
output: html_notebook
---

```{r}
library(tidyverse)
library(readxl)
library(GGally)
library(zoo)
```

```{r}
the.data <- read_excel("FOCB_CMS Data 2016_2018 CCB Edits.xlsx")
the.data <- the.data %>%
  select(-c('..19', '..20')) %>%
  mutate(DateTime = as.POSIXct(DateTime, format = "%m/%d/%Y %H:%M", tz = 'Etc/GMT-5')) %>%
  # There's some ambiguity here about how datetimes were created by FOCB
  # The data included records for times that do not exist during the transition from EDT to EST
  # (the hour of 2:00 am is dropped, yet we have data for that time in 2017)
  # See rows  5631 and 14367
  
  # Rows 14615  and 14616 correspond to real times, but appear to be empty of all data, so can be dropped.
  # To maintian even spacing of observations, I insert the missing time and date info
  # I checked the excel file data, and I found no gaps in the times. they are all spaced one hour apart
  # So I assume all times are all in EST.
  # The fix appears to specify a timezone that does not shift for dailight savings time.  That can be specified
  # in a number of ways, including as a numeral, which is hours west of GMT.  This appears to work:
  
  # a <- seq(as.POSIXct("2017-03-12 02:00:00", tz = 'Etc/GMT-5'), by = 1, length.out = 5)
  # a
  
  mutate(Date = as.POSIXct(Date, format = "%m/%d/%Y", tz='Etc/GMT-5')) %>%
  mutate(Julian = as.POSIXlt(Date)$yday)  %>%
  select (-Time)              #No point in converting Time, since it's the same as "hour" or as "DateTime"


# Now, fill in the missing date/time data
  the.data$DateTime[14615]<- as.POSIXct('2018-03-21 10:00:00', tz = 'Etc/GMT-5')
  the.data$Day[14615] <- 21
  the.data$Month[14615] <- 3
  the.data$Year[14615] <- 2018
  the.data$Hour[14615] <- 10
  
  
  the.data$DateTime[14616]<- as.POSIXct('2018-03-21 11:00:00', tz = 'Etc/GMT-5')
  the.data$Day[14616] <- 21
  the.data$Month[14616] <- 3
  the.data$Year[14616] <- 2018
  the.data$Hour[14616] <- 11
```

Could consider converting month to a factor



```{r}
which(is.na(the.data$DateTime))
```



```{r}
 table(sl <- grepl("/", OlsonNames()))

OlsonNames()[ !sl ] # the simple ones


```

```{r}
summary(the.data)

```

```{r}
the.data %>% ggpairs(7:14)
```
Depth and Temperature show strong bimodal distributions.  Why?
Salinity also shows some structure.
DO shows sme bimodal structure, but DOSAte does not -- perhaps because it is temperature corrected.
Bot hChl and pCO2 show skewness.  Chl has some VERY high outliers.

Correlations are interesting too:
DO is negatively correlated with temperature - -as expected.
pH is negatively correlated with Temp, perhaps because of its tie to DO?
pCO2 is positively correlated with temperature.
DO is Positivvely correlated with pH andnegatively correlated wit hPCO2
DOSat is similarly correlated, but less strongly.
pH is negatively correlated with pCO2
Chlorophyll is negatively 

So, lets first look at those bimodal distributions
```{r}
the.data %>% ggplot(aes(Date, Temp)) + geom_point() + geom_smooth()
```

So, that shows a pretty nice seasonal trend, slightly poorly modeled by the GAM smoother.  

```{r}
monthmeans <- the.data %>% 
        group_by(Month) %>% 
        summarise(meanTemp = mean(Temp, na.rm = TRUE),
                  minTemp = min(Temp, na.rm = TRUE),
                  maxTemp = max(Temp, na.rm = TRUE)) %>%
  mutate(MonthF = factor(Month, levels = as.character(1:12),
                                   labels = c('Jan','Feb','Mar','Apr','May','Jun',
                                              'Jul','Aug','Sep','Oct','Nov','Dec'))) %>%
  arrange(MonthF) %>%
  filter(! is.na(Month))

```


```{r}
ggplot() +
  geom_jitter(data = the.data, aes(x = Month, y = Temp, color = (Hour + 6)%%24), alpha = 0.25) +
  scale_colour_distiller( type = "div", palette = 1) +
    geom_line(data = monthmeans, aes(x = Month, y = meanTemp), color = 'aquamarine4', lwd = 2) +
    geom_line(data = monthmeans, aes(x = Month, y = minTemp), color = 'blue', lwd = 2) +
    geom_line(data = monthmeans, aes(x = Month, y = maxTemp), color = 'green', lwd = 2)

```

So, it looks like the bimodal distribution of temperatures reflects the long periods of the year that are relatively warm (July, August andSeptember) and relatively cold (Latter half of December, January, February, March).

Lets try to color code by month, and add markers for the monthly means.  The only trick here was getting the color ramps to assign consistently and correctly.  Note the use of "arrange" to reorder the dataframes.
Apparently, ggplot assigns colors in order of first appearance of a factor level.
```{r}
the.data %>% mutate(MonthF = factor(Month, levels = 1:12,
                                   labels = c('Jan','Feb','Mar','Apr','May','Jun',
                                              'Jul','Aug','Sep','Oct','Nov','Dec'))) %>% 
  arrange(MonthF) %>%
  ggplot(aes(x =Temp, fill = MonthF)) +
  scale_fill_hue(aesthetics = c("colour", "fill")) +
  geom_histogram(binwidth = 1, alpha = .5) +
  geom_segment(data = monthmeans,
               aes(x = meanTemp, y = 1500,
                   xend = meanTemp, yend = 1300,
                   color = MonthF, order = MonthF),
               lwd = 2, arrow = arrow(length = unit(0.2,"cm"))) 

```

Presumably Depth is bimodal for similar reasons, but perhaps by tidal phase, not day or month.


```{r}
the.data %>% ggplot(aes(Date, Depth)) + geom_point() + geom_smooth()
```
Of course, the problem here is that the tidal data does not follow any of the predictor time variables.  We would have to find successive high tides some how

#Calculating the Times of High Tides
Here's one way to do that.
```{r}
library(zoo)

tmp <- the.data %>% filter(Year == 2017, Month == 6)

localmax <- rollapply(tmp$Depth, 15, function (x) which.max(x)==8, align = 'center')
localmax = c(rep(FALSE,7), localmax, c(rep(FALSE,7)))
localmaxdatetime <- tmp$DateTime[localmax]
localmaxdepth <- tmp$Depth[localmax]
 ggplot() + geom_line(data = tmp, aes(x= DateTime, y= Depth) ) +
  geom_point(aes(x=localmaxdatetime, y = localmaxdepth), color = 'yellow') +
  scale_x_datetime()
 rm(tmp)
```
Pretty cool....

#Finding time since last high tide
Now that I have a list of the hours nearest successive high tides, I need to be able to calculate for any time in the record how long ago the previous high tide was.

Here's a potentialy useful function:
```{r}
findInterval(c(0, 1,2,3,4,5,6,7,8,9), c(2,4,6,8,10))
```
What does "findInterval" do?  You might think of it as a function that assigns values in the first list to bins defined in the second list.  Oyu might, for example, use this to define categories or assign values to histogram bars.

For our use, we put the list of all times in the first parameter, and the list of ONLY high tides in the second parameter, and the function will figure out which interval (defined by values in the second list) each value in the first list belongs to.   The function returns a list of the INDEXES of the "closest but smaller" value in the second list.

```{r}
localmax <- rollapply(the.data$Depth, 15, function (x) which.max(x)==8, align = 'center')
localmax <- c(rep(FALSE,7), localmax, c(rep(FALSE,7)))
the.data$hightide <- localmax     # Add a flag to the record indicating high tides

localmaxdatetime <- the.data$DateTime[localmax]
localmaxdatetime <-  sort(localmaxdatetime)  #Not sure why this needed sorting
#localmaxdepth <- the.data$Depth[localmax]


tideindexes <- findInterval(the.data$DateTime, localmaxdatetime)
tideindexes[tideindexes ==0] = NA
tidetimes =  localmaxdatetime[tideindexes]
the.data$sincehigh <- difftime(the.data$DateTime,tidetimes,units = 'hours')
```

```{r}
the.data %>% filter (Year == 2018, Month <6) %>% ggplot() + aes(sincehigh, Depth, color = hightide) + geom_point()
```


```{r}
the.data <- the.data %>% select(-c(sincehigh,hightide))
```

Let's apply that to the tides again.
```{r}
tmp <- the.data %>% filter(Year == 2017, Month == 6)
h <- function(x) ifelse(any(is.na(x)), NA, which(x == max(x))==8)
localmax <- rollapply(tmp$Depth, 15, h, align = 'center')

# Pad out the results of rollapply to match the inout vector in length and alignment
localmax = c(rep(FALSE,7), localmax, c(rep(FALSE,7)))

# Now, add datetime and depth to the data frame
localmaxdatetime <- tmp$DateTime[localmax]
localmaxdatetime <- localmaxdatetime[!is.na(localmaxdatetime)]
localmaxdepth <- tmp$Depth[localmax]
localmaxdepth <- localmaxdepth[!is.na(localmaxdepth)]
```


```{r}
# finally, figure out when the previous high tide occured
# Identify the index of the previous high tide in localmaxdatetime
tideindexes <- findInterval(tmp$DateTime, localmaxdatetime)
# Eliminate spurious values before the first high tide
tideindexes[tideindexes== 0] = NA
# Lookup the time that corresponds
tidetimes =  localmaxdatetime[tideindexes]
# Calculate a time difference
tmp$sincehigh <- difftime(tmp$DateTime,tidetimes,units = 'hours')
# Any that are more than 13 hours after a high tide must be from a period where some data were missing.
tmp$sincehigh[tmp$sincehigh>13]<- NA
```


```{r}
 ggplot() + geom_line(data = tmp, aes(x= DateTime, y= Depth) ) +
  geom_point(aes(x=localmaxdatetime, y = localmaxdepth), color = 'Red') +
  scale_x_datetime()

```

Note that there are a couple of High tides, both early and late that wew not picked out.  Those are periods with some missing data.

Can we package that all as a function?  Or should I break that into multiple functions?

```{r}

h <- function(x) ifelse(any(is.na(x)), NA, which(x == max(x))==8)

findhightides <- function (dpth, datetime, windo = 15, fxn = h) {
  #Find local maximum depths, with 
  localmax <- rollapply(dpth, windo, fxn, align = 'center')
  
  # Pad out the results of rollapply to match the input vector in length and alignment
  localmax = c(rep(FALSE,(as.integer(windo/2))),
                   localmax,
                   rep(FALSE,(as.integer(windo/2)))
                   )
  
  # Now, find corresponding datetime and depth
  localmaxdatetime <- datetime[localmax]
  localmaxdatetime <- localmaxdatetime[!is.na(localmaxdatetime)]
  
  localmaxdepth <- tmp$Depth[localmax]
  localmaxdepth <- localmaxdepth[!is.na(localmaxdepth)]
  #Return columns
  return(tibble(Depth = localmaxdepth, DateTime = localmaxdatetime))
}


sincehightide <- function (dpth, datetime, windo = 15, fxn = h) {
  #Find local maximum depths, with 
  localmax <- rollapply(dpth, windo, fxn, align = 'center')
  
  # Pad out the results of rollapply to match the inout vector oin length and alignment
  localmax = c(rep(FALSE,(as.integer(windo/2))),
                   localmax,
                   rep(FALSE,(as.integer(windo/2)))
                   )
  
  # Now, find corresponding datetime and depth
  localmaxdatetime <- datetime[localmax]
  localmaxdatetime <- localmaxdatetime[!is.na(localmaxdatetime)]
  
  localmaxdepth <- tmp$Depth[localmax]
  localmaxdepth <- localmaxdepth[!is.na(localmaxdepth)]
  
  # finally, figure out when the previous high tide occured
  # Identify the index of the previous high tide in localmaxdatetime
  tideindexes <- findInterval(datetime, localmaxdatetime)
  # Eliminate spurious values before the first high tide
  tideindexes[tideindexes== 0] <- NA
  # Lookup the correspondng times.
  tidetimes <-  localmaxdatetime[tideindexes]
  #Calculate time differences
  sincehigh <- difftime(datetime,tidetimes,units = 'hours')
  # Any that are more than 13 hours after a high tide must be from a period where some data were missing.
  sincehigh[sincehigh>13]<- NA
  return(sincehigh)
}


```

```{r}
tmp2 <- the.data %>% filter(Year == 2017, Month == 6)

tmp2$SinceHigh <- sincehightide(tmp2$Depth, tmp2$DateTime)
 ggplot() + geom_point(data = tmp2, aes(x= DateTime, y= Depth, color = as.factor(SinceHigh) ) ) +
  scale_x_datetime()
```




```{r}
the.data$sincehigh <- sincehightide(the.data$Depth, the.data$DateTime)


the.data %>% filter (Year == 2018, Month <6) %>% ggplot() + aes(sincehigh, Depth, color = hightide) + geom_point()
```




```{r}
which(is.na(the.data$DateTime))
```

#Time Iconsistencies
##Dates and Times that are out of order
```{r}
(a <- which(the.data$DateTime-lag(the.data$DateTime)<0))

the.data$DateTime[a]

```
"2016-08-24"  -- Block of "missing data" 13:00 to 23:00 appears to have been added to the data before remainder of day
"2017-01-20"  -- 13:00 is missing data, entered at beginningg of day
"2017-03-21"  -- 10:00 is missing data, entered at beginning of day
"2017-05-10"  -- 10:00 is missing data, entered at beginning of day
"2017-06-01"  -- 12:00 to 23:00 entered before 0:00 to 11:00
"2017-11-06"  -- Looks to me like the dates have been offset by one -- date changes at 23:00, not 0:00
"2017-11-07"  -- Off sequence
"2017-11-08"  -- Off sequence
"2017-11-09"  -- Off sequence
"2017-11-10"  -- Off sequence
"2017-11-11"  -- Off sequence
"2017-11-12"  -- Off sequence
"2017-11-13"  -- Off sequence
"2017-11-14"  -- Off sequence
"2017-11-15"  -- This one looks odd -- There's no 11/14/2017 23:00, and 11/15/2017 afternoon is listed before morning
"2018-06-01"  -- 10:00 to 23:00 listed before 0:00 to 9:00

```{r}
reordered.data <- the.data %>% arrange(DateTime)
```

```{r}
reordered.data$sincehigh <- sincehightide(reordered.data$Depth, reordered.data$DateTime)


reordered.data %>% filter (Year == 2018, Month <6) %>%
  ggplot() + geom_point(data = reordered.data, aes(x= DateTime, y= Depth, color = as.factor(sincehigh) ) ) +
    scale_x_datetime()
```

```{r}
reordered.data %>% ggplot(aes(x=sincehigh, y=month)) +  contour(aes(z=salinity))


```




